{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6529c083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f97d75b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load('/home/miyashita21/gitrepo/3dshapes/checkpoints/autoencoder/best-epoch=1549-val_loss=0.0052.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a832bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt['optimizer_states'][0]['param_groups'][0]['lr'] = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab34542e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(ckpt, '/home/miyashita21/gitrepo/3dshapes/checkpoints/autoencoder/best-epoch=1549-val_loss=0.0052_modified.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9d5835",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miyashita21/gitrepo/3dshapes/venv/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/home/miyashita21/gitrepo/3dshapes/venv/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torchvision as tv\n",
    "import wandb\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels=256, latent_dim=128):\n",
    "        super().__init__()\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            torch.nn.Softplus(),\n",
    "            torch.nn.LayerNorm([32, 64, 64]),\n",
    "            torch.nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            torch.nn.Softplus(),\n",
    "            torch.nn.LayerNorm([32, 64, 64]),\n",
    "            torch.nn.Conv2d(32, 64, kernel_size=4, stride=4),\n",
    "            torch.nn.Softplus(),\n",
    "            torch.nn.LayerNorm([64, 16, 16]),\n",
    "            torch.nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            torch.nn.Softplus(),\n",
    "            torch.nn.LayerNorm([64, 16, 16]),\n",
    "            torch.nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            torch.nn.Softplus(),\n",
    "            torch.nn.LayerNorm([64, 16, 16]),\n",
    "            torch.nn.Conv2d(64, 128, kernel_size=4, stride=4),\n",
    "            torch.nn.Softplus(),\n",
    "            torch.nn.LayerNorm([128, 4, 4]),\n",
    "            torch.nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            torch.nn.Softplus(),\n",
    "            torch.nn.LayerNorm([128, 4, 4]),\n",
    "            torch.nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            torch.nn.Softplus(),\n",
    "            torch.nn.LayerNorm([128, 4, 4]),\n",
    "            torch.nn.Conv2d(128, 256, kernel_size=4, stride=4),\n",
    "            torch.nn.Softplus(),\n",
    "            torch.nn.LayerNorm([256, 1, 1]),\n",
    "            torch.nn.Conv2d(256, latent_dim, kernel_size=1),\n",
    "        )\n",
    "\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose2d(latent_dim, 256, kernel_size=1),\n",
    "            torch.nn.Softplus(),\n",
    "            torch.nn.LayerNorm([256, 1, 1]),\n",
    "            torch.nn.ConvTranspose2d(256, 128, kernel_size=4, stride=4),\n",
    "            torch.nn.Softplus(),\n",
    "            torch.nn.LayerNorm([128, 4, 4]),\n",
    "            torch.nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1),\n",
    "            torch.nn.Softplus(),\n",
    "            torch.nn.LayerNorm([128, 4, 4]),\n",
    "            torch.nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1),\n",
    "            torch.nn.Softplus(),\n",
    "            torch.nn.LayerNorm([128, 4, 4]),\n",
    "            torch.nn.ConvTranspose2d(128, 64, kernel_size=4, stride=4),\n",
    "            torch.nn.Softplus(),\n",
    "            torch.nn.LayerNorm([64, 16, 16]),\n",
    "            torch.nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1),\n",
    "            torch.nn.Softplus(),\n",
    "            torch.nn.LayerNorm([64, 16, 16]),\n",
    "            torch.nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1),\n",
    "            torch.nn.Softplus(),\n",
    "            torch.nn.LayerNorm([64, 16, 16]),\n",
    "            torch.nn.ConvTranspose2d(64, 32, kernel_size=4, stride=4),\n",
    "            torch.nn.Softplus(),\n",
    "            torch.nn.LayerNorm([32, 64, 64]),\n",
    "            torch.nn.ConvTranspose2d(32, 32, kernel_size=3, padding=1),\n",
    "            torch.nn.Softplus(),\n",
    "            torch.nn.LayerNorm([32, 64, 64]),\n",
    "            torch.nn.ConvTranspose2d(32, 3, kernel_size=3, padding=1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon\n",
    "\n",
    "\n",
    "class Autoencoder(pl.LightningModule):\n",
    "    def __init__(self, model, sample_num=64):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.recon_imgs = None\n",
    "            \n",
    "    def on_validation_epoch_end(self):\n",
    "        # 検証終了時に画像生成しwandbに記録\n",
    "        if self.recon_imgs is None:\n",
    "            return\n",
    "        with torch.no_grad():\n",
    "            img = self.recon_imgs\n",
    "            grid = tv.utils.make_grid(img.cpu(), nrow=8)\n",
    "            wandb_logger = self.logger\n",
    "            if hasattr(wandb_logger, \"experiment\"):\n",
    "                wandb_logger.experiment.log({f\"val_generated/epoch_{self.current_epoch}\": wandb.Image(grid, caption=f\"epoch {self.current_epoch}\")})\n",
    "            self.recon_imgs = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        recon = self.model(batch)\n",
    "        loss = torch.nn.functional.mse_loss(recon, batch)\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        recon = self.model(batch)\n",
    "        if self.recon_imgs is None:\n",
    "            self.recon_imgs = recon.clamp(0, 1).cpu()\n",
    "        loss = torch.nn.functional.mse_loss(recon, batch)\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adamax(self.model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "        return optimizer\n",
    "\n",
    "model = Model(hidden_channels=64, latent_dim=256)\n",
    "autoencoder = Autoencoder(model).load_state_dict(ckpt['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7da7b1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['images', 'labels']>\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "# load dataset\n",
    "dataset = h5py.File('3dshapes.h5', 'r')\n",
    "print(dataset.keys())\n",
    "images = dataset['images']\n",
    "labels = dataset['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82bf2b76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.  ,   0.  ,   0.  ,   0.75,   0.  , -30.  ])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a0c0b47",
   "metadata": {},
   "outputs": [
    {
     "ename": "_LinAlgError",
     "evalue": "cusolver error: CUSOLVER_STATUS_INVALID_VALUE, when calling `cusolverDnXsyevd_bufferSize( handle, params, jobz, uplo, n, CUDA_R_32F, reinterpret_cast<const void*>(A), lda, CUDA_R_32F, reinterpret_cast<const void*>(W), CUDA_R_32F, workspaceInBytesOnDevice, workspaceInBytesOnHost)`. This error may appear if the input matrix contains NaN. If you keep seeing this error, you may use `torch.backends.cuda.preferred_linalg_library()` to try linear algebra operators with other supported backends. See https://pytorch.org/docs/stable/backends.html#torch.backends.cuda.preferred_linalg_library",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_LinAlgError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(n\u001b[38;5;241m*\u001b[39mn, n\u001b[38;5;241m*\u001b[39mn)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m      6\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m x\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m----> 8\u001b[0m l, u \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meigh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31m_LinAlgError\u001b[0m: cusolver error: CUSOLVER_STATUS_INVALID_VALUE, when calling `cusolverDnXsyevd_bufferSize( handle, params, jobz, uplo, n, CUDA_R_32F, reinterpret_cast<const void*>(A), lda, CUDA_R_32F, reinterpret_cast<const void*>(W), CUDA_R_32F, workspaceInBytesOnDevice, workspaceInBytesOnHost)`. This error may appear if the input matrix contains NaN. If you keep seeing this error, you may use `torch.backends.cuda.preferred_linalg_library()` to try linear algebra operators with other supported backends. See https://pytorch.org/docs/stable/backends.html#torch.backends.cuda.preferred_linalg_library"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "n = 192\n",
    "\n",
    "x = torch.randn(n*n, n*n).cuda()\n",
    "\n",
    "x = x + x.T\n",
    "\n",
    "l, u = torch.linalg.eigh(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
